{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report \n",
    "---\n",
    "This report describe the follow points:\n",
    "    Learning Algorithm,\n",
    "    Plot of Rewards,\n",
    "    Ideas for Future Work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to resolve the problem, we used the DDPG Algorithm, this algorithm is based in Actor-Critic, and it use multiple agents so the final is MADDPG.\n",
    "\n",
    "We have two models one for the Actor an another for the Critic, Actor-Critic work together, The Actor its inputs are the states and return the actions to execute, and the critic its inputs are states and the actions and return the quality, the QValue, so the Critic evaluate the action that return the actor.\n",
    "\n",
    "And it implements a buffer that store past tuple of state, actions rewards, etc, and it is used as experienced replay. Later we get a batch of that, so we take of this buffer and its is used to learn and we add noise to the return of actions.\n",
    "\n",
    "The algorithm is the same at Continuos Control (20 agents) so its multiagent.\n",
    "\n",
    "The modifications are:\n",
    "\n",
    "1.- I use the same hyperparameters at Continuos Control  Model 2048-1024 batch size 1024 but it doesn work (score +0.1 ), so reduced  to Model 256-124 batch 128 (used at Continuos Control, that learns slowly but more stable) and get score at +0.3 \n",
    "\n",
    "2.- So review MADPPG paper https://arxiv.org/pdf/1706.02275v4.pdf and applied Clipping Gradient  https://androidkt.com/how-to-apply-gradient-clipping-in-pytorch/ and thats the lines added and it get scored +0.7\n",
    "\n",
    "    GCLIP_VALUE = 1          # Gradient Clipping by value, Limit to be between -CLIP_VALUE -> +CLIP_VALUE\n",
    "    nn.utils.clip_grad_norm_(self.critic_local.parameters(), GCLIP_VALUE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor, inputs are states:\n",
    "\n",
    "![title](red.png)\n",
    "\n",
    "Critic, inputs are states and actions\n",
    "![title](red.png)\n",
    "\n",
    "\n",
    "The arquitecture used is the input layer, two hidden layer and a output layer and the Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hyperparameter that we use are:\n",
    "\n",
    "Model for Actor : \n",
    "\n",
    "    1 Input Layer,  size = 24\n",
    "    2 Hidden layers   first 256 nodes and second 128 nodes  \n",
    "    1 Outpur Layer size = 2 (action size)\n",
    "    Activation function = Tanh (Hyperbolic tangent)\n",
    "\n",
    "Model for Critic : \n",
    "\n",
    "    1 Input Layer,  size = states + actions\n",
    "    2 Hidden layers   first 256 nodes and second 128 nodes  \n",
    "    1 Output Layer size = 2 (action size)\n",
    "    Activation function = Relu (Rectified Linear Unit)\n",
    "\n",
    " \n",
    "Buffer size = 100000\n",
    "\n",
    "Batch Size = 128      \n",
    "\n",
    "GAMMA = 0.99         (discount factor)\n",
    "\n",
    "TAU = 1e-3           (for soft update of target parameters)\n",
    "\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "\n",
    "LR_CRITIC = 2e-4        # learning rate of the critic\n",
    "\n",
    "GCLIP_VALUE = 1          # Gradient Clipping by value, Limit to be between -CLIP_VALUE -> +CLIP_VALUE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LR of Actor and Critic values that i used is at this paper \n",
    "https://www.mdpi.com/1424-8220/19/7/1547/pdf Table 2 and Table 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plot of Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the training process many times, we get success about 1700 episodes, the score we set to succeed is +0.5 , but after some cicles we get 0.9.\n",
    "the episodes running is at 4000 to verify if the behavior of the Agent is stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained models plays at values of +0.5 but i think performs erratic and get better at +0.7. so, +0.9 is ok , so the next goal to achieve is get estable +0.9 because some times we get +0.5 on top on some runnings, and anothers +0.7 on top.\n",
    "\n",
    "\n",
    "It could be test the  DE-MADDPG https://arxiv.org/pdf/2003.10598.pdf to get that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
